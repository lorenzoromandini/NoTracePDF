---
phase: 01-mvp-foundation
plan: 08
type: execute
wave: 4
depends_on: [01-01, 01-02, 01-03, 01-04, 01-05, 01-06, 01-07]
files_modified: []
autonomous: true
requirements: [ARCH-01, ARCH-02, ARCH-03, ARCH-04, ARCH-05, ARCH-06, ARCH-07, ARCH-08, DEPLOY-01, DEPLOY-02, DEPLOY-03, DEPLOY-04]

must_haves:
  truths:
    - "Zero-trace architecture is verifiable — no files persist after processing"
    - "No user data appears in logs"
    - "Cache headers prevent browser caching of downloads"
    - "Docker container has no persistent volumes"
    - "tmpfs mounts are RAM-backed and ephemeral"
    - "Memory limits prevent host OOM from malicious PDFs"
    - "Container runs on NAS/VPS/home lab hardware"
  artifacts:
    - path: "tests/test_zero_trace.py"
      provides: "Zero-trace verification tests"
      contains: "test_no_file_persistence"
    - path: "tests/test_security.py"
      provides: "Security verification tests"
      contains: "test_cache_headers"
    - path: "scripts/verify_deployment.sh"
      provides: "Deployment verification script"
      contains: "tmpfs"
  key_links:
    - from: "tests/test_zero_trace.py"
      to: "docker inspect"
      via: "volume verification"
      pattern: "docker.*inspect"
    - from: "scripts/verify_deployment.sh"
      to: "Dockerfile"
      via: "deployment verification"
      pattern: "tmpfs"
---

<objective>
Verify the zero-trace architecture through comprehensive testing and validation. Ensure all security requirements are met before declaring MVP complete.

Purpose: The privacy guarantee must be verifiable. This plan creates tests and verification scripts that prove no user data persists.

Output: Verified deployment with proof that zero-trace guarantees hold.
</objective>

<execution_context>
@/home/ubuntu/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/research/PITFALLS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test framework and pytest configuration</name>
  <files>
    requirements.txt
    pytest.ini
    tests/__init__.py
    tests/conftest.py
  </files>
  <action>
    Set up testing infrastructure:

    1. Update `requirements.txt`:
       - pytest==8.0.0
       - pytest-asyncio==0.23.0
       - httpx==0.27.0 (for async API testing)
       - pytest-docker (optional, for container testing)

    2. Create `pytest.ini`:
       ```ini
       [pytest]
       testpaths = tests
       python_files = test_*.py
       python_functions = test_*
       asyncio_mode = auto
       ```

    3. Create `tests/conftest.py`:
       - Fixture for test client (httpx AsyncClient)
       - Fixture for sample PDF bytes
       - Fixture for sample image bytes
       - Helper functions for API calls

    4. Create sample test files in tests/fixtures/:
       - sample.pdf (minimal valid PDF)
       - sample.png (small test image)

    Reference ARCH-01 to ARCH-08: Tests verify these requirements.
  </action>
  <verify>
    pip install pytest pytest-asyncio httpx -q && pytest --collect-only 2>/dev/null | head -5
  </verify>
  <done>
    - pytest installed and configured
    - Test fixtures created
    - Sample test files exist
    - pytest can discover and run tests
  </done>
</task>

<task type="auto">
  <name>Task 2: Create zero-trace verification tests</name>
  <files>
    tests/test_zero_trace.py
  </files>
  <action>
    Create tests verifying zero-persistence:

    1. Create `tests/test_zero_trace.py`:
       - test_no_file_persistence_after_merge():
         - Upload test PDFs
         - Call merge endpoint
         - Download result
         - Verify /tmp and /app/uploads are empty
       - test_no_file_persistence_after_error():
         - Upload invalid file
         - Verify cleanup happens even on error
       - test_tmpfs_is_ram_backed():
         - Check that /tmp mount is tmpfs
         - Verify no actual disk usage
       - test_container_no_volumes():
         - Run `docker inspect` on container
         - Assert no persistent volumes exist

    2. Use subprocess to run docker commands:
       ```python
       import subprocess
       
       def test_container_no_volumes():
           result = subprocess.run(
               ["docker", "inspect", "notracepdf", "--format", "{{.Mounts}}"],
               capture_output=True
           )
           # Verify no persistent volume mounts
       ```

    3. Test cleanup on SIGTERM:
       - Send SIGTERM during processing
       - Verify cleanup runs

    Reference ARCH-02, ARCH-03, ARCH-08: Volume and persistence tests.
  </action>
  <verify>
    python -c "
import subprocess
result = subprocess.run(['pytest', '--collect-only', 'tests/test_zero_trace.py'], capture_output=True)
print('Test collection:', 'OK' if result.returncode == 0 else 'FAIL')
"
  </verify>
  <done>
    - test_no_file_persistence_after_merge exists
    - test_no_file_persistence_after_error exists
    - test_tmpfs_is_ram_backed exists
    - test_container_no_volumes exists
    - All tests pass
  </done>
</task>

<task type="auto">
  <name>Task 3: Create security verification tests</name>
  <files>
    tests/test_security.py
    tests/test_api.py
  </files>
  <action>
    Create tests verifying security requirements:

    1. Create `tests/test_security.py`:
       - test_cache_headers_on_download():
         - Make request to any download endpoint
         - Verify Cache-Control: no-store, no-cache, must-revalidate, private
         - Verify Pragma: no-cache
         - Verify Expires: 0
       - test_no_sensitive_data_in_logs():
         - Process a file with known filename
         - Read log output
         - Assert filename NOT in logs
         - Assert file size NOT in logs
       - test_file_size_limit_enforced():
         - Upload file larger than MAX_FILE_SIZE_MB
         - Verify 413 Request Entity Too Large
       - test_request_timeout():
         - Start upload that takes too long
         - Verify timeout triggers

    2. Create `tests/test_api.py`:
       - test_all_endpoints_exist():
         - GET /health returns 200
         - All POST endpoints return appropriate responses
       - test_file_type_validation():
         - Upload non-PDF to PDF endpoint
         - Verify 415 Unsupported Media Type

    Reference ARCH-04, ARCH-05, ARCH-06, ARCH-07: Security tests.
  </action>
  <verify>
    python -c "
import subprocess
result = subprocess.run(['pytest', '--collect-only', 'tests/test_security.py'], capture_output=True)
print('Security tests collected:', 'OK' if result.returncode == 0 else 'FAIL')
"
  </verify>
  <done>
    - Cache header tests exist and pass
    - Logging privacy tests exist and pass
    - File size limit tests exist and pass
    - API endpoint tests exist and pass
  </done>
</task>

<task type="auto">
  <name>Task 4: Create deployment verification script</name>
  <files>
    scripts/verify_deployment.sh
  </files>
  <action>
    Create comprehensive deployment verification script:

    1. Create `scripts/verify_deployment.sh`:
       ```bash
       #!/bin/bash
       set -e
       
       echo "=== NoTracePDF Deployment Verification ==="
       
       # Check container is running
       echo "1. Checking container status..."
       docker ps | grep notracepdf
       
       # Check tmpfs mounts
       echo "2. Verifying tmpfs mounts..."
       docker exec notracepdf mount | grep tmpfs
       
       # Check no persistent volumes
       echo "3. Verifying no persistent volumes..."
       docker inspect notracepdf | grep -v '"Volumes": {}' && echo "FAIL: Volumes found" || echo "OK: No volumes"
       
       # Check memory limits
       echo "4. Verifying memory limits..."
       docker stats --no-stream --format "{{.MemPerc}}" notracepdf
       
       # Test health endpoint
       echo "5. Testing health endpoint..."
       curl -sf http://localhost:8000/health || echo "FAIL: Health check"
       
       # Test cache headers
       echo "6. Verifying cache headers..."
       curl -sI http://localhost:8000/health | grep -i "cache-control: no-store"
       
       # Test file upload and verify cleanup
       echo "7. Testing file cleanup..."
       # Upload a test file and verify /tmp is empty after
       
       echo "=== Verification Complete ==="
       ```

    2. Make script executable

    3. Run script and verify all checks pass

    Reference DEPLOY-01 to DEPLOY-04: Deployment verification.
  </action>
  <verify>
    chmod +x scripts/verify_deployment.sh && bash -n scripts/verify_deployment.sh && echo "Script syntax OK"
  </verify>
  <done>
    - Verification script exists and is executable
    - Script checks tmpfs mounts
    - Script verifies no volumes
    - Script tests health endpoint
    - Script verifies cache headers
  </done>
</task>

<task type="auto">
  <name>Task 5: Run full test suite and verify all requirements</name>
  <files>
    tests/test_integration.py
  </files>
  <action>
    Create integration tests and run full verification:

    1. Create `tests/test_integration.py`:
       - test_full_merge_workflow():
         - Upload 2 PDFs
         - Merge them
         - Download result
         - Verify result is valid PDF
         - Verify cleanup
       - test_full_split_workflow()
       - test_full_compress_workflow()
       - test_full_password_workflow()
       - test_full_watermark_workflow()
       - test_pdf_to_images_workflow()
       - test_images_to_pdf_workflow()

    2. Run all tests:
       ```bash
       pytest tests/ -v --tb=short
       ```

    3. Generate coverage report (optional):
       ```bash
       pytest tests/ --cov=app --cov-report=term
       ```

    4. Document test results:
       - Create test run summary
       - Note any failing tests
       - Confirm all requirements have passing tests

    5. Run deployment verification:
       ```bash
       ./scripts/verify_deployment.sh
       ```

    Reference: All ARCH and DEPLOY requirements.
  </action>
  <verify>
    pytest tests/ -v --tb=short 2>&1 | tail -20
  </verify>
  <done>
    - All unit tests pass
    - All integration tests pass
    - Deployment verification passes
    - Coverage report generated
    - Test results documented
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Complete MVP with:
    - 8 core PDF operations (merge, split, rotate, compress, password, watermark, extract text/images/pages)
    - PDF ↔ Image conversion
    - Web UI with drag-drop, progress, download
    - Zero-trace architecture verified
    - Docker deployment ready
  </what-built>
  <how-to-verify>
    1. Run: `docker-compose up -d`
    2. Open: http://localhost:8000
    3. Test merge: Upload 2 PDFs, merge, download
    4. Test split: Upload PDF, split by page range
    5. Test compress: Upload PDF, compress with medium quality
    6. Test password: Add password, then remove password
    7. Test watermark: Add text watermark to PDF
    8. Test extract: Extract text and images from PDF
    9. Test PDF to images: Convert PDF to PNG
    10. Test images to PDF: Convert images to PDF
    11. Run: `./scripts/verify_deployment.sh`
    12. Verify: "Files deleted" message appears after download
  </how-to-verify>
  <resume-signal>Type "approved" or describe issues found</resume-signal>
</task>

</tasks>

<verification>
After all tasks complete:
1. `pytest tests/ -v` shows all tests passing
2. `./scripts/verify_deployment.sh` shows all checks passing
3. Manual testing of all features works
4. Zero-trace verified (no files persist, no logs with user data)
</verification>

<success_criteria>
- All unit tests pass
- All integration tests pass
- Zero-trace architecture verified (no persistent volumes, tmpfs RAM-backed)
- Cache headers verified on all responses
- No user data in logs
- Memory limits verified
- Container runs on any Docker host
- Requirements ARCH-01 to ARCH-08, DEPLOY-01 to DEPLOY-04 verified
</success_criteria>

<output>
After completion, create `.planning/phases/01-mvp-foundation/01-08-SUMMARY.md`
</output>
